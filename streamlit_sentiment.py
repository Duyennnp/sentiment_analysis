import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from underthesea import word_tokenize, pos_tag, sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
import regex
import string
import time
import joblib
from wordcloud import WordCloud, STOPWORDS
import ast
import plotly.express as px

# 1. Read data 
df = pd.read_csv("processed_content.csv")
df = df.dropna()

# file ph√¢n t√≠ch s·∫£n ph·∫©m
sp_analysis = pd.read_csv("sp_analysis.csv")

STOP_WORD_FILE = 'vietnamese-stopwords-D.txt'
with open(STOP_WORD_FILE, 'r', encoding='utf-8') as file:
    stop_words = file.read()

stop_words = stop_words.split('\n')

# chuy·ªÉn list th√†nh chu·ªói
def convert_to_list(word_list_str):
    try:
        return ast.literal_eval(word_list_str)  # Chuy·ªÉn ƒë·ªïi chu·ªói JSON-like th√†nh danh s√°ch
    except (ValueError, SyntaxError):
        return []


sp_analysis['positive_word_list'] = sp_analysis['positive_word_list'].apply(convert_to_list)
sp_analysis['negative_word_list'] = sp_analysis['negative_word_list'].apply(convert_to_list)

sp_analysis['positive_words'] = sp_analysis['positive_word_list'].apply(lambda x: ' '.join(x))
sp_analysis['negative_words'] = sp_analysis['negative_word_list'].apply(lambda x: ' '.join(x))

# file s√¥ l∆∞·ª£ng b√¨nh lu·∫≠n
danhgia_sp_tg = pd.read_csv('danhgia_sp_tg.csv')

# 2. Chia t·∫≠p d·ªØ li·ªáu
X = df['processed_content']
y = df['sentiment']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh ƒë·∫∑c tr∆∞ng (Bag-of-Words)
vectorizer = CountVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)

# 4. Load models 
# ƒê·ªçc model Random Forest ƒë∆∞·ª£c ch·ªçn
# T·∫£i m√¥ h√¨nh b·∫±ng joblib
sentiment_model = joblib.load('model.pkl')


# 5. function c·∫ßn thi·∫øt
# D·ª± ƒëo√°n c·∫£m ƒë√°nh gi√° cho b√¨nh lu·∫≠n
def predict_sentiment(comments, model, vectorizer):
    """
    H√†m d·ª± ƒëo√°n c·∫£m x√∫c cho m·ªôt ho·∫∑c nhi·ªÅu b√¨nh lu·∫≠n v√† th√™m emoji bi·ªÉu c·∫£m.
    
    Parameters:
    - comments: Danh s√°ch c√°c b√¨nh lu·∫≠n ho·∫∑c DataFrame ch·ª©a c·ªôt b√¨nh lu·∫≠n.
    - model: M√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán.
    - vectorizer: Vectorizer ƒë√£ hu·∫•n luy·ªán
    
    Returns:
    - DataFrame v·ªõi c·ªôt b√¨nh lu·∫≠n, d·ª± ƒëo√°n v√† emoji t∆∞∆°ng ·ª©ng.
    """
    # N·∫øu input l√† danh s√°ch (m·ªôt ho·∫∑c nhi·ªÅu b√¨nh lu·∫≠n)
    if isinstance(comments, list):
        comments_df = pd.DataFrame(comments, columns=["Comment"])
    elif isinstance(comments, pd.DataFrame):
        comments_df = comments.rename(columns={comments.columns[0]: "Comment"})
    else:
        raise ValueError("Input ph·∫£i l√† danh s√°ch c√°c b√¨nh lu·∫≠n ho·∫∑c DataFrame.")

    # Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n sang vector
    comments_vec = vectorizer.transform(comments_df["Comment"])

    # D·ª± ƒëo√°n c·∫£m x√∫c
    predictions = model.predict(comments_vec)
    comments_df["Prediction"] = predictions

    # Th√™m c·ªôt emoji d·ª±a tr√™n d·ª± ƒëo√°n
    emoji_mapping = {
        "t√≠ch c·ª±c": "üòä",  # M·∫∑t c∆∞·ªùi vui v·∫ª
        "trung t√≠nh": "üòê",   # M·∫∑t b√¨nh th∆∞·ªùng
        "ti√™u c·ª±c": "üòû"   # M·∫∑t bu·ªìn
    }
    comments_df["Emoji"] = comments_df["Prediction"].map(emoji_mapping)

    return comments_df


# H√†m ƒë·ªÉ cung c·∫•p th√¥ng tin li√™n quan s·∫£n ph·∫©m
def generate_product_report(ma_san_pham, data):
    # L·∫•y th√¥ng tin s·∫£n ph·∫©m c·ª• th·ªÉ
    product_info = data[data['ma_san_pham'] == ma_san_pham]
    
    if product_info.empty:
        st.write(f"S·∫£n ph·∫©m v·ªõi m√£ s·∫£n ph·∫©m {ma_san_pham} kh√¥ng t·ªìn t·∫°i.")
        return
    
    ten_san_pham = product_info['ten_san_pham'].values[0]
    positive_count = product_info['positive_count'].values[0]
    negative_count = product_info['negative_count'].values[0]
    positive_word_list = product_info['positive_word_list'].values[0]
    negative_word_list = product_info['negative_word_list'].values[0]
    positive_words = product_info['positive_words'].values[0]
    negative_words = product_info['negative_words'].values[0]
    processed_content = product_info['processed_content'].values[0]

    # Wordcloud cho nh·∫≠n x√©t t√≠ch c·ª±c
    fig, ax = plt.subplots(figsize=(10, 5))
    positive_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(positive_words)
    ax.imshow(positive_wordcloud, interpolation='bilinear')
    ax.axis('off') 
    ax.set_title(f"Wordcloud for Positive Comments - m√£ s·∫£n ph·∫©m {ma_san_pham}")
    st.pyplot(fig)
    plt.close(fig)

    # Wordcloud cho nh·∫≠n x√©t ti√™u c·ª±c
    fig, ax = plt.subplots(figsize=(10, 5))
    negative_wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='Reds').generate(negative_words)
    ax.imshow(negative_wordcloud, interpolation='bilinear')
    ax.axis('off') 
    ax.set_title(f"Wordcloud for Negative Comments - m√£ s·∫£n ph·∫©m {ma_san_pham}")
    st.pyplot(fig)
    plt.close(fig)

    # Wordcloud cho n·ªôi dung b√¨nh lu·∫≠n (n·∫±m ·ªü gi·ªØa h√†ng d∆∞·ªõi)
    fig, ax = plt.subplots(figsize=(10, 5))
    content_wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words).generate(processed_content)
    ax.imshow(content_wordcloud, interpolation='bilinear')
    ax.axis('off') 
    ax.set_title(f"Wordcloud cho m√£ s·∫£n ph·∫©m {ma_san_pham}")
    st.pyplot(fig)
    plt.close(fig)
    
    # T·∫°o DataFrame ch·ª©a th√¥ng tin s·∫£n ph·∫©m
    report_df = pd.DataFrame({
        'ma_san_pham': [ma_san_pham],
        'ten_san_pham': [ten_san_pham],
        'positive_count': [positive_count],
        'negative_count': [negative_count],
        'positive_word_list': [positive_word_list],
        'negative_word_list': [negative_word_list],
        'processed_content': [processed_content]
    })
    
    return report_df

# T·∫°o danh s√°ch s·∫£n ph·∫©m s·∫Ω ƒë∆∞a v√†o selectbox
product_ids = [422217292, 422216990, 422220606, 422220469, 422222535, 422219399, 
                   100190059, 100150058, 100240016, 100230059, 100230064, 100230057, 100220035]

filtered_data = sp_analysis[sp_analysis['ma_san_pham'].isin(product_ids)][['ma_san_pham', 'ten_san_pham']]


# H√†m ƒë·ªÉ tr·ª±c quan h√≥a s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo ng√†y
def visualize_product_info(df, product_code):
    # L·ªçc d·ªØ li·ªáu theo m√£ s·∫£n ph·∫©m
    product_data = df[df['ma_san_pham'] == product_code]

    # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu cho m√£ s·∫£n ph·∫©m n√†y
    if product_data.empty:
        print(f"No data found for product code: {product_code}")
        return

    # T√≠nh to√°n s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo th√°ng v√† s·ªë sao trung b√¨nh theo th√°ng
    comment_count = product_data.groupby('thang_binh_luan').size().reset_index(name='so_luong_binh_luan')
    average_stars = product_data.groupby('thang_binh_luan')['so_sao'].mean().reset_index(name='so_sao_trung_binh')

    # H·ª£p nh·∫•t hai DataFrame
    merged_data = pd.merge(comment_count, average_stars, on='thang_binh_luan')

    # V·∫Ω bi·ªÉu ƒë·ªì t∆∞∆°ng t√°c b·∫±ng Plotly
    fig = px.bar(merged_data, x='thang_binh_luan', y='so_luong_binh_luan',
                 hover_data={'thang_binh_luan': '|%Y-%m', 'so_luong_binh_luan': True, 'so_sao_trung_binh': True},
                 labels={'so_luong_binh_luan': 'S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n', 'thang_binh_luan': 'Th√°ng b√¨nh lu·∫≠n'},
                 title=f'S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n v√† s·ªë sao theo th√°ng b√¨nh lu·∫≠n cho s·∫£n ph·∫©m {product_code}')
    
    # Th√™m ƒë∆∞·ªùng bi·ªÉu ƒë·ªì cho s·ªë sao trung b√¨nh
    fig.add_scatter(x=merged_data['thang_binh_luan'].astype(str), y=merged_data['so_sao_trung_binh'], mode='lines+markers',
                    name='S·ªë sao trung b√¨nh', yaxis='y2')

    # T√πy ch·ªânh tr·ª•c y2 cho s·ªë sao trung b√¨nh
    fig.update_layout(
        yaxis2=dict(
            title='S·ªë sao trung b√¨nh',
            overlaying='y',
            side='right'
        )
    )

    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
    st.plotly_chart(fig)




#---------------------------------------------------------------------------
# Th√™m CSS t√πy ch·ªânh
st.markdown(
    """
    <style>
    /* N·ªÅn cho to√†n b·ªô ·ª©ng d·ª•ng */
    .stApp {
        background: linear-gradient(to top, #FFFFFF, #98FB98); /* Xanh l√° ƒë·∫øn tr·∫Øng */
        color: black; /* M√†u ch·ªØ tr·∫Øng */
    }

    /* T√πy ch·ªânh ti√™u ƒë·ªÅ */
    h1, h2, h3, h4, h5, h6 {
        color: black;
    }

    /* T√πy ch·ªânh ph·∫ßn sidebar */
    .css-1d391kg { /* M√£ l·ªõp cho sidebar */
        background: #333333; /* M√†u n·ªÅn sidebar */
        color: black; /* M√†u ch·ªØ trong sidebar */
    }
    </style>
    """,
    unsafe_allow_html=True
)

# GUI
# T·ª±a ƒë·ªÅ ch√≠nh
st.title("üåü Project 1: Sentiment Analysis")

# Menu v·ªõi c√°c m·ª•c v√† icon
st.sidebar.image("hasaki1.jpg")
menu = ["üè¢ Business Objective", "üìä Build Project", "üìà New Prediction", "üõí Product Analysis"]
choice = st.sidebar.selectbox('üìÇ Menu', menu)

# Sidebar th√¥ng tin
st.sidebar.markdown("""
    ### üßë‚Äçüíª Th√†nh vi√™n th·ª±c hi·ªán:
    - Nguy·ªÖn Ng·ªçc Ph∆∞∆°ng Duy√™n
    - Mai Anh S∆°n
""")
st.sidebar.markdown("""
    ### üìö Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:
    - Khu·∫•t Th√πy Ph∆∞∆°ng
""")
st.sidebar.write("### üè´ M√£ l·ªõp: DL07_299T27_ON")
st.sidebar.write("### üóìÔ∏è Th·ªùi gian th·ª±c hi·ªán: 12/2024")


# N·ªôi dung t·ª´ng m·ª•c
if choice == 'üè¢ Business Objective':
    st.subheader("üè¢ Business Objective")
    st.image("hasaki2.jpg")
    st.markdown("""
    ### T·ªïng quan v·ªÅ HASAKI:
    - HASAKI.VN l√† h·ªá th·ªëng c·ª≠a h√†ng m·ªπ ph·∫©m ch√≠nh h√£ng v√† d·ªãch v·ª• chƒÉm s√≥c s·∫Øc ƒë·∫πp chuy√™n s√¢u v·ªõi h·ªá th·ªëng c·ª≠a h√†ng tr·∫£i d√†i tr√™n to√†n qu·ªëc.
    - Kh√°ch h√†ng c√≥ th·ªÉ l√™n ƒë√¢y ƒë·ªÉ l·ª±a ch·ªçn s·∫£n ph·∫©m, xem c√°c ƒë√°nh gi√°/ nh·∫≠n x√©t c≈©ng nh∆∞ ƒë·∫∑t mua s·∫£n ph·∫©m.
    """)
    st.write("#### üìå M·ª•c ti√™u:")
    st.write("#### 1Ô∏è‚É£ **X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m**")
    st.image("sentiment.jpg", caption="Ph√¢n t√≠ch ph·∫£n h·ªìi kh√°ch h√†ng")
    st.write("#### 2Ô∏è‚É£ **Th·ª±c hi·ªán ph√¢n t√≠ch s·∫£n ph·∫©m**")
    st.image("product_analysis.jpg", caption="Ph√¢n t√≠ch s·∫£n ph·∫©m Hasaki")

elif choice == 'üìä Build Project':
    st.subheader("Build Project")
    st.write("#### 1. Xem d·ªØ li·ªáu")
    st.dataframe(df.head(3))
    st.dataframe(df.tail(3))  # Replace with `df.head()` for real data
    
    st.write("#### 2. Visualize Sentiment: üìâ")
    st.image("SoLuong_BinhLuan.jpg", caption="S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo ph√¢n lo·∫°i")
    st.markdown("""
    - D·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng v·ªõi l·ªõp t√≠ch c·ª±c chi·∫øm ƒëa s·ªë.
    - ‚úÖ Ph∆∞∆°ng √°n kh·∫Øc ph·ª•c: Ti·∫øn h√†nh train tr√™n t·∫≠p hi·ªán t·∫°i v√† c·∫£i thi·ªán n·∫øu c·∫ßn.
    """)
    st.image("Imbalance.jpg", caption="C√°c ph∆∞∆°ng ph√°p kh·∫Øc ph·ª•c m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu")

    st.write("#### 3. X√¢y d·ª±ng m√¥ h√¨nh Random Forest")
    st.write("Training time: ‚è±Ô∏è 17.519s")
    st.write("#### 4. ƒê√°nh gi√° m√¥ h√¨nh:üìä")
    st.markdown("""
    **C√°c ch·ªâ s·ªë m√¥ h√¨nh:**
    - Accuracy: üéØ 0.980
    - Precision: ‚úÖ 0.980
    - Recall: üîÅ 0.980
    - F1 score: üìà 0.980
    """)
    st.image("classification_report_v0.jpg", caption="Classification Report")
    st.image("confusion_matrix_rfr_graph.jpg", caption="Confusion Matrix")
    
    st.write("#### 5. Summary: M√¥ h√¨nh Random Forest v·ªõi accuracy cao 0.98 cho th·∫•y m√¥ h√¨nh nh·∫≠n di·ªán kh√° t·ªët cho c√°c l·ªõp.")

elif choice == 'üìà New Prediction':
    st.image('hasaki_banner_1.jpg', use_container_width=True)
    st.write("### D·ª± ƒëo√°n c·∫£m x√∫c b√¨nh lu·∫≠n kh√°ch h√†ng")
    st.markdown("""
    - ‚≠ê **<3 sao**: Ti√™u c·ª±c
    - ‚≠ê **=3 sao**: Trung t√≠nh
    - ‚≠ê **>3 sao**: T√≠ch c·ª±c
    """)
    st.write("#### L·ª±a ch·ªçn lo·∫°i d·ªØ li·ªáu ƒë∆∞a v√†o:")
    flag = False
    lines = None
    type = st.radio("##### L·ª±a ch·ªçn lo·∫°i d·ªØ li·ªáu:", options=("üìÇ Upload", "üñäÔ∏è Input"))
    if type == "üìÇ Upload":
        uploaded_file_1 = st.file_uploader("Ch·ªçn file:", type=['txt', 'csv'])
        if uploaded_file_1:
            st.write("‚úÖ File uploaded th√†nh c√¥ng.")
            if uploaded_file_1 is not None:
                lines = pd.read_csv(uploaded_file_1, header=None, sep='\t')
                st.dataframe(lines)            
                comments = lines.iloc[:, 0].tolist()     
                flag = True                          
    elif type == "üñäÔ∏è Input":
        input_text = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n:")
        if st.button("D·ª± ƒëo√°n üîç"):
            st.write("‚úÖ K·∫øt qu·∫£ d·ª± ƒëo√°n s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y.")
            if input_text !="":
                if input_text.strip():
                    comments = input_text.split("\n") #lines = [input_text .strip()]
                    flag = True
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi emoji
    if flag:
        st.write("###### Content:")
        if len(comments) > 0:
            st.code(comments)
        
            # Ghi nh·∫≠n th·ªùi gian b·∫Øt ƒë·∫ßu
            start_time = time.time()
        
            # Th·ª±c hi·ªán d·ª± ƒëo√°n
            results = predict_sentiment(comments, sentiment_model, vectorizer)
        
            # Ghi nh·∫≠n th·ªùi gian k·∫øt th√∫c
            end_time = time.time()
        
            # T√≠nh th·ªùi gian d·ª± ƒëo√°n
            prediction_time = end_time - start_time
        
             # Hi·ªÉn th·ªã th·ªùi gian d·ª± ƒëo√°n
            st.write(f"**Th·ªùi gian d·ª± ƒëo√°n**: {prediction_time:.3f} gi√¢y")
        
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi emoji tr√™n giao di·ªán
            for _, row in results.iterrows():
                comment = row["Comment"]
                prediction = row["Prediction"]
                emoji = row["Emoji"]
                st.write(f"**B√¨nh lu·∫≠n**: {comment}")
                st.write(f"**D·ª± ƒëo√°n**: {prediction} {emoji}")
                st.markdown("---")      
           
    
elif choice == 'üõí Product Analysis':
    st.image('product.jpg', use_container_width=True)
    st.subheader("üõí Product Analysis")
    
    # T·∫°o m·ªôt danh s√°ch m√£ s·∫£n ph·∫©m v√† t√™n s·∫£n ph·∫©m t·ª´ DataFrame 
    product_options = [f"{row['ma_san_pham']} - {row['ten_san_pham']}" for index, row in filtered_data.iterrows()]
    
    # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn ho·∫∑c nh·∫≠p m√£ s·∫£n ph·∫©m 
    selected_product_option = st.selectbox("üîç Ch·ªçn s·∫£n ph·∫©m:", product_options)

    # L·∫•y m√£ s·∫£n ph·∫©m t·ª´ t√πy ch·ªçn ƒë√£ ch·ªçn 
    selected_product_id = int(selected_product_option.split(' - ')[0]) 
    st.write("##### B·∫°n ƒë√£ ch·ªçn:", selected_product_option)

    # Hi·ªÉn th·ªã b√°o c√°o cho m√£ s·∫£n ph·∫©m ƒë√£ ch·ªçn 
    if st.button("üìä Hi·ªÉn th·ªã b√°o c√°o"):
        st.write(f"‚úÖ B√°o c√°o s·∫£n ph·∫©m {selected_product_option} s·∫Ω hi·ªÉn th·ªã t·∫°i ƒë√¢y.")
        report_df = generate_product_report(selected_product_id, sp_analysis) 
        
        # Hi·ªÉn th·ªã b√°o c√°o m√† kh√¥ng s·ª≠ d·ª•ng DataFrame
        st.write("### üîç List Positive:")
        for index, row in report_df.iterrows():
            st.write(f"- **S·∫£n ph·∫©m**: {row['ten_san_pham']}")
            st.write(f"  - **S·ªë l∆∞·ª£ng t√≠ch c·ª±c**: {row['positive_count']}")
            st.write(f"  - **Danh s√°ch t·ª´ kh√≥a t√≠ch c·ª±c**: {row['positive_word_list']}")
            st.markdown("---")
        
        st.write("### üî¥ List Negative:")
        for index, row in report_df.iterrows():
            st.write(f"- **S·∫£n ph·∫©m**: {row['ten_san_pham']}")
            st.write(f"  - **S·ªë l∆∞·ª£ng ti√™u c·ª±c**: {row['negative_count']}")
            st.write(f"  - **Danh s√°ch t·ª´ kh√≥a ti√™u c·ª±c**: {row['negative_word_list']}")
            st.markdown("---")
        
        SL_BinhLuan = visualize_product_info(danhgia_sp_tg, selected_product_id)